{
  "hash": "ead13521b1dee262489e5356707d9129",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Procesos Basados en Vectores Autoregresivos\nsubtitle: Descubre cómo seleccionar hardware, descargar la imagen ISO y preparar los medios de instalación. Exploraremos opciones para probar o instalar Linux en tu equipo.\ndescription: |\n  Únete a esta emocionante serie de introducción a Linux, donde te guiaré a través de los pasos para descargar e instalar GNU/Linux en tu equipo. Aprenderás a seleccionar el hardware adecuado, descargar la imagen ISO de tu distribución preferida y preparar los medios de instalación. Además, exploraremos diferentes opciones para probar o instalar Linux. ¡Embárcate en esta aventura y descubre el poder de GNU/Linux!\ncategories:\n  - Informática\n  - Tecnología\n  - Sistemas Operativos\n  - Linux\ntags:\n  - Linux\n  - OpenSource\n  - GNU/Linux\n  - SistemasOperativos\n  - DistribucionesLinux\n  - DescargaDeLinux\n  - InstalaciónDeLinux\n  - Hardware\n  - ImagenISO\ndate: \"08/27/2023\"\n---\n\n\n\n\n\n\n# Procesos Basados en Vectores Autoregresivos\n\n\\chapter{Procesos basados en Vectores Autoregresivos}\n\nEn este capítulo removeremos el supuesto de que el análisis es univariado, ya que introduciremos la posibilidad de que los procesos generadores de datos compartan información entre dos o más series. Como primer aproximación desarrollaremos el concepto de Causalidad de Granger. Mediante esta metodología discutiremos cuando dos series se causan estadísticamente. Posteriormente, introduciremos una técnica más sofisticada conocida como la metodología de Vectores Autoregresivos (VAR), la cual es una generalización de los procesos Autoregresivos (AR) que analizamos al principio del curso.\n\n## Causalidad de Granger\n\nHasta ahora hemos supuesto que una serie puede ser explicada únicamente con la información contenida en ella misma. No obstante, en adelante trataremos de analizar el caso en el que buscamos determinar relaciones entre variables y cómo el comportamiento de una serie influye en las demás. Algunas relaciones más importantes son las llamadas causalidad. En este caso analizaremos el procedimiento de Granger (1969), conocido como causalidad de Granger. En adelante asumiremos que las series involucradas son debílmente estacionarias.\n\nSean $X$ y $Y$ dos series debílmente estacionarias. Definamos a $I_t$ un conjunto de toda la información disponible hasta el momento $t$. Asimismo, digamos que $\\overline{X}_t$ y $\\overline{Y}_t$ son los conjuntos de toda la información disponible (actual y pasada) de $X$ y $Y$, respectivamente. Es decir:\n\\begin{eqnarray*}\n    \\overline{X}_t & := & \\{ X_t, X_{t-1}, X_{t-2}, \\ldots \\} \\\\\n    \\overline{Y}_t & := & \\{ Y_t, Y_{t-1}, Y_{t-2}, \\ldots \\} \\\\\n    I_t & := & \\overline{X}_t + \\overline{Y}_t\n\\end{eqnarray*}\n\nAdicionalmnete, definamos $\\sigma^2(*)$ como la varianza del término de error estimado de una regresión dada. Dicho lo anterior, digamos que:\n\\begin{enumerate}\n\n\\item Existe Causalidad de Granger o $X$ causa a $Y$ si y solo si, una regresión lineal da como resultado que: \n    \\begin{equation}\n        \\sigma^2 (Y_{t+1} | I_t) < \\sigma^2 (Y_{t+1} | I_t - X_t)    \n    \\end{equation}\n\nEs decir, que la variabilidad del término de error de una regresión lineal de $Y$ sobre el conjunto de toda la información aplicada a un pronóstico de $Y_{t+1}$ es MENOR que la variabilidad del término de error de una regresión lineal de $Y$ sobre el conjunto de la información de $Y$ aplicada a un pronóstico de $Y_{t+1}$.\n\n\\item Existe Causalidad de Granger Instantanéa o $X$ causa de forma instantanéa a $Y$ si y solo si, una regresión lineal da como resultado:\n    \\begin{equation}\n        \\sigma^2 (Y_{t+1} | \\{ I_t, X_{t+1} \\}) < \\sigma^2 (Y_{t+1} | I_t)\n    \\end{equation}\n\n\\end{enumerate}\n\nLa definición anterior aplica de igual forma si se reemplaza a $X$ por $Y$ y a $Y$ por $X$, respectivamente. De acuerdo a la definición anterior, existen 5 diferentes posibilidades de relaciones causales entre las dos series:\n\\begin{enumerate}\n\\item $X$ y $Y$ son independientes: $(X, Y)$;\n\n\\item Existe solo causalidad instantanéa: $(X - Y)$;\n\n\\item $X$ causa a $Y$: $(X \\longrightarrow Y)$;\n\n\\item $Y$ causa a $X$: $(X \\longleftarrow Y)$, y\n\n\\item Ambas series se causan: $(X \\longleftrightarrow Y)$.\n\n\\end{enumerate}\n\nPor lo anterior, representaremos mediante un $AR(p)$ con variables exógenas lo siguiente:\n\\begin{equation}\n    A(L) \n    \\begin{bmatrix}\n    Y_t \\\\ X_t\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    a_{11}(L) & a_{12}(L) \\\\ a_{21}(L) & a_{22}(L)\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    Y_t \\\\ X_t\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    V_t \\\\ U_t\n    \\end{bmatrix}\n    (\\#eq:GrangerEq)\n\\end{equation}\n\nO en su versión $MA(q)$  con variables exógenas:\n\\begin{equation}\n    \\begin{bmatrix}\n    Y_t \\\\ X_t\n    \\end{bmatrix}\n    =\n    B(L)\n    \\begin{bmatrix}\n    V_t \\\\ U_t\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    b_{11}(L) & b_{12}(L) \\\\ b_{21}(L) & b_{22}(L)\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    V_t \\\\ U_t\n    \\end{bmatrix}\n\\end{equation}\n\nPara determinar el test de causalidad utilizaremos una especificación similar a la de la ecuación \\@ref(eq:GrangerEq). Para probar si $X$ causa a $Y$ consideraremos la siguiente regresión:\n\\begin{equation}\n    Y_t = \\alpha_0 + \\sum^{k_1}_{k = 1} a^k_{11} Y_{t-k} + \\sum^{k_2}_{k = k_0} a^k_{12} X_{t-k} + U_{1,t}\n\\end{equation}\n\nDonde $k_0 = 1$ y, en general, se asume que $k_1 = k_2$. Asimismo, el valor de estas constantes se puede determinar con el cirterio de Akaike (o cualquier otro criterio de información). No obstante, algunos autores sugieren que una buena práctica es considerar valores de $k_1$ y $k_2$ 4, 8, 12 y 16.\n\nDicho lo anterior, el test de causalidad de Granger se establece con una prueba F (similar a la definiada en el Apéndice de estas notas), en la cual se prueba la siguiente hipótesis nula:\n\\begin{equation}\n    H_0: a^1_{12} = a^2_{12} = \\ldots = a^{k2}_{12} = 0\n\\end{equation}\n\nAhora veámos un ejemplo. Consideremos como variables analizadas al Índice Nacional de Precios al Consumidor ($INPC_t$), al Tipo de Cambio ($TDC_t$) y al rendimiento anual de los Cetes a 28 días ($CETE28_t$), todas desestacionalizadas para el periodo de enero de 2000 a julio de 2019. Dado que la metodología de Granger supone que las series son estacionarias, utilizaremos las diferencias logaritmicas de cada una de las tres series (es decir, utilizaremos una transformación del tipo $ln(X_t) - ln(X_{t-1})$). La Figura \\@ref(fig:fig61) muestra las series en su transformación de diferencias logarítmicas.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stats)\nlibrary(MASS)\nlibrary(strucchange)\nlibrary(zoo)\nlibrary(sandwich)\nlibrary(urca)\nlibrary(lmtest)\nlibrary(vars)\n\n#\nload(\"Datos_Ad.RData\")\n\n#\nINPC <- ts(Datos_Ad$INPC_Ad, \n           start = c(2000, 1), \n           freq = 12)\n\nDLINPC <- ts(log(Datos_Ad$INPC_Ad) - lag(log(Datos_Ad$INPC_Ad), 1), \n             start = c(2000, 1), \n             freq = 12)\n\nTC <- ts(Datos_Ad$TC_Ad, \n         start = c(2000, 1), \n         freq = 12)\n\nDLTC <- ts(log(Datos_Ad$TC_Ad) - lag(log(Datos_Ad$TC_Ad), 1), \n           start = c(2000, 1), \n           freq = 12)\n\nCETE28 <- ts(Datos_Ad$CETE28_Ad, \n             start = c(2000, 1), \n             freq = 12)\n\nDLCETE28 <- ts(log(Datos_Ad$CETE28_Ad) - lag(log(Datos_Ad$CETE28_Ad), 1), \n               start = c(2000, 1), \n               freq = 12)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#\n#png(\"DLGranger.png\", width = 800, height = 1200)\n\npar(mfrow=c(3, 1))\n\nplot(DLINPC, xlab = \"Tiempo\", \n     main = \"Diferencias Logarítmicas del INPC\",\n     col = \"darkgreen\")\n\nplot(DLTC, xlab = \"Tiempo\", \n     main = \"Diferencias Logarítmicas del Tipo de Cambio\",\n     col = \"darkblue\")\n\nplot(DLCETE28, xlab = \"Tiempo\", \n     main = \"Diferencias Logarítmicas de los Cetes a 28 dias\",\n     col = \"darkred\")\n```\n\n::: {.cell-output-display}\n![Series en diferencias logarítmicas dadas por las siguientes expresiones: $DLINPC_t = ln(DLINPC_t) - ln(DLINPC_{t-1})$, $DLTC_t = ln(TC_t) - ln(TC_{t-1})$ y $DLCETE28_t = ln(CETE28_t) - ln(CETE28_{t-1})$.](index_files/figure-html/fig61-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1, 1))\n\n#dev.off()\n```\n:::\n\n\n\n\n\n\nPor simplicidad, en el Cuadro \\@ref(tab:Granger) se muestra el resultado de aplicar el test de Granger a diferentes especificaciones, con rezagos 4, 8, 12 y 16, sólo para la serie de Tipo de Cambio en diferencias logarítmicas. En cada una de las pruebas se compara el modelo considerado como regresor a la variable que es candidata de causar, respecto del modelo si considerar a dicha variable.\n\nTable: (\\#tab:Granger) Prueba de si $DLINPC_t$ Granger causa a $DLTC_t$.\n\n| Rezagos | Estadiística F | Probabilidad ($>$F) | Significancia |\n|:---:|:---:|:---:|:---:|\n| 4 | 3.2621 | 0.01265 | * |\n| 8 | 1.9079 | 0.06030 |  |\n| 12 | 2.2577 | 0.01067 | * |\n| 16 | 1.6735 | 0.05495 | * |\n| Notas: | *** | signif. | al 0.1\\% |\n|  |  ** | signif. | al 1\\% |\n|  | * |  signif. | al 5\\% |\n\nDe acuerdo con el Cuadro \\@ref(tab:Granger), podemos concluir que existe información estadísticamente significativa para concluir que la inflación causa a la tasa de depreciación cambiaria, ambas medidas como las diferencias logaritmicas. El resto de los resultados para las otras combinaciones de causalidad se encuentran en el R Markdown llamado Clase 13 ubicado en el repositorio de GitHub.\n\n## Definición y representación del Sistema o Modelo VAR(p)\n\nEn esta sección ampliaremos la discusión planteada en el apartado anterior. En el sentido de que en la sección pasada nuestra discusión se limito al análisis de causalidad entre dos variables a la vez, que si bien es posible extenderlo a más variables es un procedimiento limitado a casos particulares por las siguientes razones.\n\nEl procediento de causalidad de Granger supone que es posible identificar un sistema de ecuaciones que debe conformarse una vez que se ha identificado el sentido de la causalidad. Así, el proceso anterior necesita del conocimiento previo de las relaciones que existen entre las varibles. \n\nAdicionalmente, no resuleve el problema más general qué esta relacionado con cómo identificar la causalidad cuando se tienen múltiples variables con múltiples sentidos de causalidad. En esta sección analizaremos una mejor aproximación al probelma de cómo identificar la causalidad múltiple. Por lo tanto, como mécanismo para solucionar el problema planteado, analizaremos el caso de un Sistema o Modelo de Vectores Autoregresivos conocido como VAR.\n\nEl primer supuesto del que partiremos es que existe algún grado de endogenidad entre las variables considerdas en el análisis. Adicionalmente, el segundo supuesto que estableceremos es que requerimos que las variables que tengamos consideradas sean estacionarias.\n\nPor lo anterior diremos que un VAR es un procedimiento que sigue fundado en el supuesto de que las variables consideredas son estacionarias, sin que hasta el momento hallamos podido establecer un mécanismo de detección de dicha estacionariedad. Así, hasta este momento del curso hemos pasado de modelo univariados a modelo múltivariados, pero no hemos podido dejar de asumir que las series son estacionarias.\n\nAhora bien, iniciaremos con el establecimiento de la representación del proceso. Digamos que tenemos un proceso estocástico $\\mathbf{X}$ estacionario de dimensión $k$. De esta forma la expresión reducida del modelo o el proceso $VAR(p)$ estará dado por:\n\\begin{equation}\n    \\mathbf{X}_t = \\mathbf{\\delta} + A_1 \\mathbf{X}_{t-1} + A_2 \\mathbf{X}_{t-2} + \\ldots + A_p \\mathbf{X}_{t-p} + \\mathbf{U}_{t}\n    (\\#eq:VARp)\n\\end{equation}\n\nDonde cada uno de las $A_i$, $i = 1, 2, \\ldots, p$, son matrices cuadradas de dimensión $k$ y $\\mathbf{U}_t$ representa un vector de dimensión $k \\times 1$ con los residuales en el momento del tiempo $t$ que son un proceso pueramente aleatorio. También se incorpora un vector de términos constantes denominado como $\\mathbf{\\delta}$, el cual es de dimensión $k \\times 1$.\n\nAsí, la ecuación \\@ref(eq:VARp) supone la siguiente estructura de vectores:\n\\begin{equation*}\n    \\mathbf{X}_t = \n    \\begin{bmatrix}\n    X_{1t} \\\\ X_{2t} \\\\ \\vdots \\\\ X_{kt}\n    \\end{bmatrix}\n\\end{equation*}\n\nPara cualquier $i = 1, 2, \\ldots, p$:\n\\begin{equation*}\n    \\mathbf{X}_{t-i} = \n    \\begin{bmatrix}\n    X_{1t-i} \\\\ X_{2t-i} \\\\ \\vdots \\\\ X_{kt-i}\n    \\end{bmatrix}\n\\end{equation*}\n\n\\begin{equation*}\n    \\mathbf{\\delta} = \n    \\begin{bmatrix}\n    \\delta_{1} \\\\ \\delta_{2} \\\\ \\vdots \\\\ \\delta_{k}\n    \\end{bmatrix}\n\\end{equation*}\n\nTambién, la ecuación \\@ref(eq:VARp) supone que cada matriz $A_i$, $i = 1, 2, \\ldots, p$, esta definida de la siguiente forma:\n\\begin{equation*}\n    \\mathbf{A}_i = \n    \\begin{bmatrix}\n    a^{(i)}_{11} & a^{(i)}_{12} & \\ldots & a^{(i)}_{1k} \\\\ a^{(i)}_{21} & a^{(i)}_{22} & \\ldots & a^{(i)}_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a^{(i)}_{k1} & a^{(i)}_{k2} & \\ldots & a^{(i)}_{kk}\n    \\end{bmatrix}\n\\end{equation*}\n\nRetomando la ecuación \\@ref(eq:VARp) y considerando que podemos ocupar el operador rezago $L^j$ de forma analóga al caso del modelo $AR(p)$, pero aplicado a un vector, tenemos las siguientes ecuaciones:\n\\begin{eqnarray}\n    \\mathbf{X}_t - A_1 \\mathbf{X}_{t-1} - A_2 \\mathbf{X}_{t-2} - \\ldots - A_p \\mathbf{X}_{t-p} & = & \\mathbf{\\delta} + \\mathbf{U}_{t} \\nonumber \\\\\n    \\mathbf{X}_t - A_1 L \\mathbf{X}_{t} - A_2 L^2 \\mathbf{X}_{t} - \\ldots - A_p L^p \\mathbf{X}_{t-p} & = & \\mathbf{\\delta} + \\mathbf{U}_{t} \\nonumber \\\\\n    (I_k - A_1 L - A_2 L^2 - \\ldots - A_p L^p) \\mathbf{X}_t & = & \\mathbf{\\delta} + \\mathbf{U}_{t} \\nonumber \\\\\n    \\mathbf{A}(L) \\mathbf{X}_t & = & \\mathbf{\\delta} + \\mathbf{U}_{t}\n    (\\#eq:VARCorto)\n\\end{eqnarray}\n\nAdicionalmente, requeriremos que dado que $\\mathbf{U}_t$ es un proceso pueramente aleatorio, este debe cumplir con las siguientes condiciones:\n\n1. El valor esperado del término de error es cero:\n  \\begin{equation}\n      \\mathbb{E}[\\mathbf{U}_t] = 0\n  \\end{equation}\n\n2. Existe una matriz de varianzas y covarianzas entre los términos de error contemporáneos dada por:\n  \\begin{eqnarray}\n      \\mathbb{E}[\\mathbf{U}_t \\mathbf{U}_t'] \n      & = &\n      \\mathbb{E} \\left[\n      \\begin{bmatrix}\n      U^{(t)}_{1} \\\\ U^{(t)}_{2} \\\\ \\vdots \\\\ U^{(t)}_{k}\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      U^{(t)}_{1} & U^{(t)}_{2} & \\ldots & U^{(t)}_{k}\n      \\end{bmatrix}\n      \\right] \\nonumber \\\\\n      & = & \\mathbb{E}\n      \\begin{bmatrix}\n      U^{(t)}_{1} U^{(t)}_{1} & U^{(t)}_{1} U^{(t)}_{2} & \\ldots & U^{(t)}_{1} U^{(t)}_{k} \\\\\n      U^{(t)}_{2} U^{(t)}_{1} & U^{(t)}_{2} U^{(t)}_{2} & \\ldots & U^{(t)}_{2} U^{(t)}_{k} \\\\\n      \\vdots & \\vdots & \\ldots & \\vdots \\\\\n      U^{(t)}_{k} U^{(t)}_{1} & U^{(t)}_{k} U^{(t)}_{2} & \\ldots & U^{(t)}_{k} U^{(t)}_{k}\n      \\end{bmatrix} \\nonumber \\\\\n      & = & \\begin{bmatrix}\n      \\sigma^2_1 & \\rho_{12} & \\ldots & \\rho_{1k} \\\\\n      \\rho_{21} & \\sigma^2_2 & \\ldots & \\rho_{2k} \\\\\n      \\vdots & \\vdots & \\ldots & \\vdots \\\\\n      \\rho_{k1} & \\rho_{k2} & \\ldots & \\sigma^2_k\n      \\end{bmatrix} \\nonumber \\\\\n      & = & \\mathbf{\\Sigma}_{UU}\n      (\\#eq:SigmaVAR)\n  \\end{eqnarray} \n\n3. La matriz de varianzas y covarianzas no comtemporáneas es nula. Es decir, que para todo $t \\neq s$:\n   \\begin{eqnarray}\n       \\mathbb{E} [\\mathbf{U}_t \\mathbf{U}_s'] \n       & = &\n       \\mathbb{E} \\left[\n       \\begin{bmatrix}\n       U^{(t)}_{1} \\\\ U^{(t)}_{2} \\\\ \\vdots \\\\ U^{(t)}_{k}\n       \\end{bmatrix}\n       \\begin{bmatrix}\n       U^{(s)}_{1} & U^{(s)}_{2} & \\ldots & U^{(s)}_{k}\n       \\end{bmatrix}\n       \\right] \\nonumber \\\\\n       & =  & \\mathbb{E}\n       \\begin{bmatrix}\n       U^{(t)}_{1} U^{(s)}_{1} & U^{(t)}_{1} U^{(s)}_{2} & \\ldots & U^{(t)}_{1} U^{(s)}_{k} \\\\\n       U^{(t)}_{2} U^{(s)}_{1} & U^{(t)}_{2} U^{(s)}_{2} & \\ldots & U^{(t)}_{2} U^{(s)}_{k} \\\\\n       \\vdots & \\vdots & \\ldots & \\vdots \\\\\n       U^{(t)}_{k} U^{(s)}_{1} & U^{(t)}_{k} U^{(s)}_{2} & \\ldots & U^{(t)}_{k} U^{(s)}_{k}\n       \\end{bmatrix} \\nonumber \\\\\n       & = & \\mathbf{0}\n       (\\#eq:RhoVAR)\n   \\end{eqnarray}\n\nLas ecuaciones \\@ref(eq:SigmaVAR) y \\@ref(eq:RhoVAR) significan que los residuales $\\mathbf{U}_t$ pueden estar correlacionados entre ellos solo en el caso de que la información sea contemporánea, pero no tienen información en común entre residuales de otros periodos.\n\nAl igual que en el caso del modelo o especificación $AR(p)$ en la especificación del modelo $VAR(p)$ existen condiciones de estabilidad. Dichas condiciones están dadas por lo siguiente, definamos el siguiente polinomio que resulta de tomar la matriz $\\mathbf{A}(L)$ en la ecuación \\@ref(eq:VARCorto):\n\\begin{equation}\n    Det[I_t - A_1 z - A_2 z^2 - \\ldots - A_p z^p] \\neq 0\n\\end{equation}\n\nDonde las raíces del polinomio cumplen que $|z| \\leq 1$, es decir, se ubican dentro del circulo unitario.\n\nLa ecuación \\@ref(eq:VARCorto) puede ser rexpresada en una forma similar al un proceso de MA. Al respecto, de forma similar a la siguiente ecuación podemos construir un modelo $VARMA(p,q)$, el cual no estudiamos es este curso. \n\nReromando el primer planteamiento, podemos escribir:\n\\begin{eqnarray}\n    \\mathbf{X}_t & = & \\mathbf{A}^{-1}(L) \\delta + \\mathbf{A}^{-1}(L) \\mathbf{U}_t \\nonumber \\\\\n    & = & \\mu + \\beta(L) \\mathbf{U}_t\n    (\\#eq:VARMAq)\n\\end{eqnarray}\n\nPor el lado de las matrices que representan la autocovarianza, estás resultan de resolver lo siguiente:\n\\begin{equation}\n    \\Gamma_X(\\tau) = E[(\\mathbf{X}_t - \\mu)(\\mathbf{X}_{t-\\tau} - \\mu)'] \n\\end{equation}\n\nAhora, sin pérdida de generalidad digamos que la especificación VAR(p) en la ecuación \\@ref(eq:VARp) no tiene constante, por lo que $\\delta = 0$, lo que implica que $\\mu = 0$. De esta forma las matrices de autocovarianza resultan de:\n\\begin{eqnarray*}\n    \\Gamma_X(\\tau) & = & E[(\\mathbf{X}_t)(\\mathbf{X}_{t-\\tau})'] \\\\\n    & = & A_1 E[(\\mathbf{X}_{t-1})(\\mathbf{X}_{t-\\tau})'] + A_2 E[(\\mathbf{X}_{t-2})(\\mathbf{X}_{t-\\tau})'] \\\\\n    &   & + \\ldots + A_p E[(\\mathbf{X}_{t-p})(\\mathbf{X}_{t-\\tau})'] + E[(\\mathbf{U}_t(\\mathbf{X}_{t-\\tau})']\n\\end{eqnarray*}\n\nFinalmente, al igual que en el caso $AR(p)$ requerimos de una métrica que nos permita determinar el número de rezagos óptimo $p$ en el $VAR(p)$. Así, establecemos criterios de información similares a los del $AR(p)$ dados por:\n\n1.Final Prediction Error (FPE):\n        \\begin{equation}\n        FPE(p) = \\left[ \\frac{T + kp + 1}{T - kp - 1} \\right]^k |\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)|\n        \\end{equation}\n    \n2. Akaike Criterion (AIC):\n        \\begin{equation}\n        AIC(p) = ln|\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| + (k + p k^2) \\frac{2}{T}\n        \\end{equation}\n    \n3. Hannan - Quinn Criterion (HQ):\n        \\begin{equation}\n        HQ(p) = ln|\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| + (k + p k^2) \\frac{2ln(ln(2))}{T}\n        \\end{equation}\n    \n4. Schwartz Criterion (SC):\n        \\begin{equation}\n        SC(p) = ln|\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| + (k + p k^2) \\frac{ln(T)}{T}\n        \\end{equation}\n        \n  Donde la matriz de varianzas y covarianzas contemporáneas estará dada por:\n    \\begin{equation*}\n            \\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p) = \\mathbb{E} \\left[\n            \\begin{bmatrix}\n            U^{(t)}_{1} \\\\ U^{(t)}_{2} \\\\ \\vdots \\\\ U^{(t)}_{k}\n            \\end{bmatrix}\n            \\begin{bmatrix}\n            U^{(t)}_{1} & U^{(t)}_{2} & \\ldots & U^{(t)}_{k}\n            \\end{bmatrix}\n            \\right]\n        \\end{equation*}\n\nAhora veámos un ejemplo de estimación de $VAR(p)$. Para el ejemplo utilizaremos las series de INPC, Tipo de Cambio, rendimiento de los Cetes a 28 días, el IGAE y el Índice de Producción Industrial de los Estados Unidos, todas desestacionalizadas y para el período de enero de 2000 a julio de 2019. Dado que el supuesto estacionariedad sigue presente en nuestro análisis, emplearemos cada una de las series en su versión de diferencias logaritmicas. Las Figuras \\@ref(fig:fig62) y \\@ref(fig:fig63) muestra las series referidas.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stats)\nlibrary(MASS)\nlibrary(strucchange)\nlibrary(zoo)\nlibrary(sandwich)\nlibrary(urca)\nlibrary(lmtest)\nlibrary(vars)\n\n#\nload(\"Datos_Ad.RData\")\n\n#\nDLINPC <- ts(log(Datos_Ad$INPC_Ad) - lag(log(Datos_Ad$INPC_Ad), 1), \n             start = c(2000, 1), \n             freq = 12)\n\nDLTC <- ts(log(Datos_Ad$TC_Ad) - lag(log(Datos_Ad$TC_Ad), 1), \n           start = c(2000, 1), \n           freq = 12)\n\nDLCETE28 <- ts(log(Datos_Ad$CETE28_Ad) - lag(log(Datos_Ad$CETE28_Ad), 1), \n               start = c(2000, 1), \n               freq = 12)\n\nDLIGAE <- ts(log(Datos_Ad$IGAE_Ad) - lag(log(Datos_Ad$IGAE_Ad), 1), \n             start = c(2000, 1), \n             freq = 12)\n\nDLIPI <- ts(log(Datos_Ad$IPI_Ad) - lag(log(Datos_Ad$IPI_Ad), 1), \n            start = c(2000, 1), \n            freq = 12)\n\nDatos <- data.frame(cbind(DLINPC, DLTC, DLCETE28, DLIGAE, DLIPI))\n\nDatos <- ts(Datos[2 : 282, ], \n            start = c(2000, 2), freq = 12)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(Datos, plot.type = \"s\", \n     col = c(\"darkgreen\", \"darkblue\", \"darkred\", \"black\", \"purple\"), \n     main = \"Series en Diferencias logaritmicas\", \n     xlab = \"Tiempo\", ylab = \"Variacion\")\n\nlegend(\"bottomright\", c(\"INPC\", \"TC\", \"CETES28\", \"IGAE\", \"IPI\"),\n       cex = 0.6, lty = 1:1, \n       col = c(\"darkgreen\", \"darkblue\", \"darkred\", \"black\", \"purple\"))\n```\n\n::: {.cell-output-display}\n![Series en diferencias logarítmicas (Forma 1)](index_files/figure-html/fig62-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(Datos, plot.type = \"m\", \n     col = \"darkgreen\", \n     main = \"Series en Diferencias logaritmicas\", xlab = \"Tiempo\")\n```\n\n::: {.cell-output-display}\n![Series en diferencias logarítmicas (Forma 2)](index_files/figure-html/fig63-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\nDicho lo anterior, a continuación mostraremos la tabla que resume el valor de los distintos criterios de información una especificación de un $VAR(p)$ con constante. Notése que es posible especificar un $VAR(p)$ con tendencia, caso que no aplica hasta este momento, ya que nuestro análisis de estacionariedad es claro respecto a la media constante (más adelante relajaremos este supuesto), lo cual elimina la poisiblidad de incluir una tendencia.\n\nEn el Cuadro \\@ref(tab:SelectVAR) reportamos los resultados de aplicar una prueba de criterios de información para diferentes valores de reagos. Del cual se concluye que el número óptimo de residuales es 2 (según el crietrio AIC y el FPE) y 1 (según el criterio HQ y el SC). Recordemos que es común que el criterio AIC siempre reporte el mayor valor de rezagos, por lo que es una buena práctica utilizarlo como referente principal.\n\nTable: (\\#tab:SelectVAR) Criterios de información para diferentes especificaciones de modelos VAR(p) con término constante de la series $DLINPC_t$, $DLTC_t$, $DLCETE28_t$, $DLIGAE_t$ y $DLIPI_t$.\n\n| Rezagos | AIC | HQ | SC | FPE |\n|:---:|:---:|:---:|:---:|:---:|\n| 1 | -4.636412e+01 | -4.617847e+01 | -4.590430e+01 | 7.317262e-21 |\n| 2 | -4.639541e+01 | -4.605506e+01 | -4.555241e+01 | 7.094216e-21 |\n| 3 | -4.635305e+01 | -4.585799e+01 | -4.512686e+01 | 7.407479e-21 |\n| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ |\n\nDe esta forma, justificamos la estimación de un $VAR(2)$. Los resultados del mismo se repotartan en los siguientes cuadros, en los que se reporta el resultado de una de las ecuaciones. Los resultados restantes se encuentran en el Scrip Clase 14 que se encuentra en repositorio de GitHub. Primero mostraremos los resutlados de las raíces del polinomio caracteristico en el Cuadro \\@ref(tab:RootsVAR), seguido de un cuadro para la ecuación del IGAE en el Cuadro \\@ref(tab:IGAE_VAR) (por simplicidad se omiten las otras cuatro ecuaciones del VAR(2)), y del Cuadro \\@ref(tab:SigmaVARp) con la matriz $\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}$ estimada del VAR.\n\nTable: (\\#tab:RootsVAR) Raíces del polinomio característico de un VAR(2).\n\n| Rezagos | AIC | HQ | SC | FPE |\n|:---:|:---:|:---:|:---:|:---:|\n| 0.7452 | 0.4403 | 0.4403 | 0.3503 | 0.3503 |\n| 0.3342 | 0.3342 | 0.3339 | 0.3339 | 0.06951 |\n\nTable: (\\#tab:IGAEVAR) Criterios de información para diferentes especificaciones de modelos VAR(p) con término constante de la series $DLINPC_t$, $DLTC_t$, $DLCETE28_t$, $DLIGAE_t$ y $DLIPI_t$.\n\n| Variable | Coeficiente | Error Est. | Estad. t | Prob.($>$ t) |\n|:---:|:---:|:---:|:---:|:---:|\n| $DLINPC_{t-1}$ | -0.2584978 | 0.1658396 | -1.559 | 0.120493 |\n| $DLTC_{t-1}$ | 0.0022016 | 0.0152876 | 0.144 | 0.885620 |\n| $DLCETE28_{t-1}$ | 0.0009547 | 0.0049115 | 0.194 | 0.846054 |\n| $DLIGAE_{t-1}$ | -0.2351453 | 0.0699797 | -3.360 | 0.000917 *** |\n| $DLIPI_{t-1}$ | 0.2442406 | 0.0600502 | 4.067 | 6.62e-05 *** |\n| $DLINPC_{t-2}$ | -0.0775039 | 0.1694809 | -0.457 | 0.647904 |\n| $DLTC_{t-2}$ | -0.0413316 | 0.0144650 | -2.857 | 0.004680 ** |\n| $DLCETE28_{t-2}$ | 0.0005341 | 0.0048058 | 0.111 | 0.911612 |\n| $DLIGAE_{t-2}$ | -0.0646890 | 0.0693711 | -0.933 | 0.352092 | \n| $DLIPI_{t-2}$ | 0.1796286 | 0.0620861 | 2.893 | 0.004195 ** |\n| $\\delta_4$ | 0.0030377 | 0.0008077 | 3.761 | 0.000217 *** |\n|  | Notas: | *** | signif. | al 0.1\\% |\n|  |  |  ** | signif. | al 1\\% |\n|  |  | * |  signif. | al 5\\% |\n\nTable: (\\#tab:SigmaVARp) Matriz $\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}$ estimada del VAR(2).\n\n|  |  |  |  |  |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | $DLINPC_t$ | $DLTC_t$ | $DLCE28_t$ | $DLIGAE_t$ | $DLIGAE_t$ | \n| $DLINPC_t$ | 3.95e-06 | 3.19e-06 | -1.83e-06 | -5.29-07 | 1.34e-06 |\n| $DLTC_t$ | 3.19e-06 | 5.04e-04 | 4.27e-04 | 9.81e-06 | 1.61e-05 |\n| $DLCE28_t$ | -1.83e-06 | 4.27e-04 | 4.63e-03 | 1.26e-05 | 2.76e-05 |\n| $DLIGAE_t$ | -5.29e-07 | 9.81e-06 | 1.26e-05 | 2.43e-05 | 8.75e-06 |\n| $DLIGAE_t$ | 1.34e-06 | 1.61e-05 | 2.76e-05 | 8.75e-06 | 3.13e-05 |\n\nFinalmente, en el Cuadro \\@ref(tab:DiagnosVAR) reportamos las pruebas de diagnóstico del VAR(2). Incluímos las pruebas de normalidad, autocorrelación parcial y de heterocedásticidad.\n\nTable: (\\#tab:DiagnosVAR) Pruebas de diagnóstico sobre los residuales del VAR(2).\n\n| Estadística (rezagos) | Coeficiente | p-value | Conclusión |\n|:---:|:---:|:---:|:---:|\n| Correlación Serial ($\\chi^2 (2)$) | 59.436 | 0.1696 | Existe autocorrelación serial |\n| Correlación Serial ($\\chi^2 (4)$) | 127.17 | 0.03461 | No existe autocorrelación serial |\n| Correlación Serial ($\\chi^2 (6)$) | 183.14 | 0.03393 | No existe autocorrelación serial |\n| Normalidad - JB ($\\chi^2$) | 2335 | 0.0000 | Los residuales no son normales |\n| ARCH ($\\chi^2 (2)$) | 691.58 | 0.0000 | Los residuales no son homocedásticos |\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nVARselect(Datos, lag.max = 12, type = \"const\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     2      1      1      2 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -4.343694e+01 -4.348783e+01 -4.342685e+01 -4.338688e+01 -4.329316e+01\nHQ(n)  -4.327594e+01 -4.319266e+01 -4.299751e+01 -4.282338e+01 -4.259549e+01\nSC(n)  -4.303604e+01 -4.275285e+01 -4.235779e+01 -4.198374e+01 -4.155594e+01\nFPE(n)  1.366449e-19  1.298899e-19  1.381225e-19  1.438821e-19  1.582460e-19\n                   6             7             8             9            10\nAIC(n) -4.324317e+01 -4.329021e+01 -4.324194e+01 -4.322014e+01 -4.317881e+01\nHQ(n)  -4.241133e+01 -4.232420e+01 -4.214176e+01 -4.198580e+01 -4.181030e+01\nSC(n)  -4.117187e+01 -4.088482e+01 -4.050247e+01 -4.014660e+01 -3.977118e+01\nFPE(n)  1.667106e-19  1.595179e-19  1.680601e-19  1.726236e-19  1.810365e-19\n                  11            12\nAIC(n) -4.311914e+01 -4.305022e+01\nHQ(n)  -4.161646e+01 -4.141338e+01\nSC(n)  -3.937743e+01 -3.897444e+01\nFPE(n)  1.936459e-19  2.093856e-19\n```\n\n\n:::\n\n```{.r .cell-code}\nVAR_p <- VAR(Datos, p = 2, type = \"const\")\n\nsummary(VAR_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVAR Estimation Results:\n========================= \nEndogenous variables: DLINPC, DLTC, DLCETE28, DLIGAE, DLIPI \nDeterministic variables: const \nSample size: 279 \nLog Likelihood: 4142.895 \nRoots of the characteristic polynomial:\n 0.53  0.53 0.4501 0.4501 0.4425 0.4425 0.3251 0.3251 0.1677 0.1677\nCall:\nVAR(y = Datos, p = 2, type = \"const\")\n\n\nEstimation results for equation DLINPC: \n======================================= \nDLINPC = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const \n\n             Estimate Std. Error t value Pr(>|t|)    \nDLINPC.l1    0.387061   0.063100   6.134 3.06e-09 ***\nDLTC.l1     -0.004611   0.005286  -0.872   0.3839    \nDLCETE28.l1  0.001229   0.002078   0.592   0.5546    \nDLIGAE.l1   -0.022088   0.015077  -1.465   0.1441    \nDLIPI.l1     0.009878   0.019363   0.510   0.6104    \nDLINPC.l2   -0.006707   0.063621  -0.105   0.9161    \nDLTC.l2      0.009608   0.005496   1.748   0.0816 .  \nDLCETE28.l2  0.001827   0.002051   0.891   0.3737    \nDLIGAE.l2    0.006544   0.014289   0.458   0.6473    \nDLIPI.l2    -0.015245   0.019632  -0.777   0.4381    \nconst        0.002325   0.000294   7.908 6.88e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.002176 on 268 degrees of freedom\nMultiple R-Squared: 0.1783,\tAdjusted R-squared: 0.1476 \nF-statistic: 5.815 on 10 and 268 DF,  p-value: 6.289e-08 \n\n\nEstimation results for equation DLTC: \n===================================== \nDLTC = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const \n\n             Estimate Std. Error t value Pr(>|t|)    \nDLINPC.l1   -1.344803   0.745382  -1.804  0.07233 .  \nDLTC.l1      0.315786   0.062442   5.057  7.9e-07 ***\nDLCETE28.l1 -0.039912   0.024542  -1.626  0.10506    \nDLIGAE.l1    0.379306   0.178102   2.130  0.03411 *  \nDLIPI.l1    -0.514827   0.228727  -2.251  0.02521 *  \nDLINPC.l2    0.071779   0.751538   0.096  0.92398    \nDLTC.l2     -0.183813   0.064926  -2.831  0.00499 ** \nDLCETE28.l2  0.030125   0.024228   1.243  0.21481    \nDLIGAE.l2    0.070713   0.168797   0.419  0.67561    \nDLIPI.l2    -0.166610   0.231909  -0.718  0.47312    \nconst        0.006463   0.003473   1.861  0.06388 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.0257 on 268 degrees of freedom\nMultiple R-Squared: 0.1425,\tAdjusted R-squared: 0.1105 \nF-statistic: 4.454 on 10 and 268 DF,  p-value: 8.102e-06 \n\n\nEstimation results for equation DLCETE28: \n========================================= \nDLCETE28 = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const \n\n             Estimate Std. Error t value Pr(>|t|)  \nDLINPC.l1    3.933798   1.861561   2.113   0.0355 *\nDLTC.l1      0.112997   0.155945   0.725   0.4693  \nDLCETE28.l1  0.113300   0.061292   1.849   0.0656 .\nDLIGAE.l1   -0.588758   0.444803  -1.324   0.1868  \nDLIPI.l1     1.174496   0.571237   2.056   0.0407 *\nDLINPC.l2   -2.213087   1.876936  -1.179   0.2394  \nDLTC.l2      0.117661   0.162151   0.726   0.4687  \nDLCETE28.l2  0.063398   0.060509   1.048   0.2957  \nDLIGAE.l2   -0.237974   0.421565  -0.565   0.5729  \nDLIPI.l2     1.110203   0.579183   1.917   0.0563 .\nconst       -0.007514   0.008674  -0.866   0.3872  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.06419 on 268 degrees of freedom\nMultiple R-Squared: 0.08823,\tAdjusted R-squared: 0.05421 \nF-statistic: 2.593 on 10 and 268 DF,  p-value: 0.005122 \n\n\nEstimation results for equation DLIGAE: \n======================================= \nDLIGAE = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const \n\n             Estimate Std. Error t value Pr(>|t|)    \nDLINPC.l1    0.719855   0.393617   1.829   0.0685 .  \nDLTC.l1     -0.218508   0.032974  -6.627 1.88e-10 ***\nDLCETE28.l1  0.007718   0.012960   0.596   0.5520    \nDLIGAE.l1    0.037559   0.094051   0.399   0.6900    \nDLIPI.l1     0.389894   0.120785   3.228   0.0014 ** \nDLINPC.l2   -0.737653   0.396868  -1.859   0.0642 .  \nDLTC.l2      0.073830   0.034286   2.153   0.0322 *  \nDLCETE28.l2 -0.013542   0.012794  -1.058   0.2908    \nDLIGAE.l2   -0.142845   0.089138  -1.603   0.1102    \nDLIPI.l2    -0.154835   0.122465  -1.264   0.2072    \nconst        0.001637   0.001834   0.893   0.3729    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01357 on 268 degrees of freedom\nMultiple R-Squared: 0.3272,\tAdjusted R-squared: 0.3021 \nF-statistic: 13.03 on 10 and 268 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation DLIPI: \n====================================== \nDLIPI = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const \n\n             Estimate Std. Error t value Pr(>|t|)    \nDLINPC.l1    0.042383   0.326611   0.130   0.8968    \nDLTC.l1     -0.163742   0.027361  -5.985 6.93e-09 ***\nDLCETE28.l1  0.012143   0.010754   1.129   0.2598    \nDLIGAE.l1    0.098821   0.078041   1.266   0.2065    \nDLIPI.l1     0.067742   0.100224   0.676   0.4997    \nDLINPC.l2   -0.524835   0.329309  -1.594   0.1122    \nDLTC.l2      0.053353   0.028449   1.875   0.0618 .  \nDLCETE28.l2 -0.016237   0.010616  -1.529   0.1273    \nDLIGAE.l2   -0.088483   0.073964  -1.196   0.2326    \nDLIPI.l2    -0.082982   0.101618  -0.817   0.4149    \nconst        0.002361   0.001522   1.551   0.1220    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01126 on 268 degrees of freedom\nMultiple R-Squared: 0.1978,\tAdjusted R-squared: 0.1679 \nF-statistic:  6.61 on 10 and 268 DF,  p-value: 3.715e-09 \n\n\n\nCovariance matrix of residuals:\n             DLINPC       DLTC  DLCETE28     DLIGAE      DLIPI\nDLINPC    4.735e-06 -1.939e-07 4.559e-06  6.652e-06  7.000e-06\nDLTC     -1.939e-07  6.607e-04 3.698e-04 -3.544e-05 -3.174e-05\nDLCETE28  4.559e-06  3.698e-04 4.121e-03  6.362e-05  8.525e-05\nDLIGAE    6.652e-06 -3.544e-05 6.362e-05  1.842e-04  1.164e-04\nDLIPI     7.000e-06 -3.174e-05 8.525e-05  1.164e-04  1.268e-04\n\nCorrelation matrix of residuals:\n            DLINPC      DLTC DLCETE28   DLIGAE   DLIPI\nDLINPC    1.000000 -0.003466  0.03264  0.22524  0.2856\nDLTC     -0.003466  1.000000  0.22409 -0.10159 -0.1097\nDLCETE28  0.032640  0.224095  1.00000  0.07301  0.1179\nDLIGAE    0.225240 -0.101585  0.07301  1.00000  0.7615\nDLIPI     0.285629 -0.109656  0.11792  0.76147  1.0000\n```\n\n\n:::\n\n```{.r .cell-code}\n### Diagnostic tests\n\n#### Normalidad:\n\nnormality.test(VAR_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$JB\n\n\tJB-Test (multivariate)\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 36186, df = 10, p-value < 2.2e-16\n\n\n$Skewness\n\n\tSkewness only (multivariate)\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 1261, df = 5, p-value < 2.2e-16\n\n\n$Kurtosis\n\n\tKurtosis only (multivariate)\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 34925, df = 5, p-value < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n#### Autocorrelacion Serial:\n\n#### LAGS = 2:\n\nserial.test(VAR_p, lags.bg = 2, type = \"BG\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBreusch-Godfrey LM test\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 74.839, df = 50, p-value = 0.013\n```\n\n\n:::\n\n```{.r .cell-code}\n#### LAGS = 4:\n\nserial.test(VAR_p, lags.bg = 4, type = \"BG\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBreusch-Godfrey LM test\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 131.69, df = 100, p-value = 0.01849\n```\n\n\n:::\n\n```{.r .cell-code}\n#### LAGS = 6:\n\nserial.test(VAR_p, lags.bg = 6, type = \"BG\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBreusch-Godfrey LM test\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 201.63, df = 150, p-value = 0.003147\n```\n\n\n:::\n\n```{.r .cell-code}\n#### Homocedasticidad:\n\narch.test(VAR_p, lags.multi = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tARCH (multivariate)\n\ndata:  Residuals of VAR object VAR_p\nChi-squared = 2122.4, df = 1350, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Análisis de Impulso-Respuesta\n\nUna de las grandes ventajas que aporta el análisis de los modelos VAR es el análisis de Impulso-Respuesta. Dicho análisis busca cuantificar el efecto que tiene en $\\mathbf{X}_t$ una innovación o cambio en los residuales de cualquiera de las variables en un momento definido. Partamos dela ecuación \\@ref(eq:VARMAq) de forma que tenemos:\n\\begin{eqnarray}\n    \\mathbf{X}_t & = & \\mathbf{A}^{-1}(L) \\delta + \\mathbf{A}^{-1}(L) \\mathbf{U}_t \\nonumber \\\\\n    & = & \\mu + \\mathbf{B}(L) \\mathbf{U}_t \\nonumber \\\\\n    & = & \\mu + \\Psi_0 \\mathbf{U}_t + \\Psi_1 \\mathbf{U}_{t-1} + \\Psi_2 \\mathbf{U}_{t-2} + \\Psi_3 \\mathbf{U}_{t-3} + \\ldots\n\\end{eqnarray}\n\nDonde $\\Psi_0 = I$ y cada una de las $\\Psi_i = - \\mathbf{B}_i$, $i = 1, 2, \\ldots$. De esta forma se verifica el efecto que tiene en $\\mathbf{X}_t$ cada las innovaciones pasadas. Por lo que el análisis de Impulso-Respuesta cuantifica el efecto de cada una de esas matrices en las que hemos descompuesto a $\\mathbf{B}(L)$.\n\nRetomando el modelo $VAR(2)$ anteriormente estimado, en las siguientes figuras reportamos las gráficas de Impulso-respuesta de la serie $DLTC_t$ ante cambios en los residuales del resto de las series y de la propia serie.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nIR_DLINPC <- irf(VAR_p, n.ahead = 12, boot = TRUE, \n                 ci = 0.95, response = \"DLINPC\")\n\nIR_DLINPC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImpulse response coefficients\n$DLINPC\n             DLINPC\n [1,]  2.175900e-03\n [2,]  8.094393e-04\n [3,]  2.429936e-04\n [4,]  1.049000e-04\n [5,]  4.305193e-05\n [6,]  2.571953e-05\n [7,]  1.084055e-05\n [8,]  1.147158e-06\n [9,] -1.425663e-06\n[10,] -9.999499e-07\n[11,] -3.443770e-07\n[12,] -1.512886e-07\n[13,] -1.157354e-07\n\n$DLTC\n             DLINPC\n [1,]  0.000000e+00\n [2,] -8.268371e-05\n [3,]  3.117547e-04\n [4,]  2.648975e-04\n [5,]  5.871629e-05\n [6,] -2.894914e-05\n [7,] -1.054637e-05\n [8,]  1.057859e-05\n [9,]  7.621634e-06\n[10,] -4.038885e-07\n[11,] -2.113820e-06\n[12,] -4.691794e-07\n[13,]  3.771599e-07\n\n$DLCETE28\n             DLINPC\n [1,]  0.000000e+00\n [2,]  6.486929e-05\n [3,]  1.318264e-04\n [4,]  3.487944e-05\n [5,]  4.740702e-05\n [6,]  3.336371e-05\n [7,]  1.571610e-05\n [8,]  3.770470e-06\n [9,]  5.672766e-08\n[10,]  2.556950e-07\n[11,]  3.379406e-07\n[12,]  4.626880e-08\n[13,] -1.137662e-07\n\n$DLIGAE\n             DLINPC\n [1,]  0.000000e+00\n [2,] -2.115678e-04\n [3,] -1.790075e-04\n [4,] -1.453804e-05\n [5,]  4.387550e-05\n [6,]  1.348413e-05\n [7,] -6.605412e-06\n [8,] -3.449275e-06\n [9,]  2.238652e-06\n[10,]  2.066817e-06\n[11,]  8.258756e-08\n[12,] -4.659518e-07\n[13,] -9.698431e-08\n\n$DLIPI\n             DLINPC\n [1,]  0.000000e+00\n [2,]  7.038188e-05\n [3,] -1.107780e-04\n [4,] -3.218008e-05\n [5,] -1.697303e-05\n [6,]  4.856595e-07\n [7,]  1.045292e-05\n [8,]  4.695418e-06\n [9,]  3.599640e-07\n[10,] -5.104865e-07\n[11,]  5.294854e-08\n[12,]  2.664766e-07\n[13,]  9.015073e-08\n\n\nLower Band, CI= 0.95 \n$DLINPC\n             DLINPC\n [1,]  1.907362e-03\n [2,]  5.149602e-04\n [3,] -7.514845e-05\n [4,] -8.251084e-05\n [5,] -5.843559e-05\n [6,] -2.647695e-05\n [7,] -1.631187e-05\n [8,] -1.904985e-05\n [9,] -1.775361e-05\n[10,] -7.521772e-06\n[11,] -3.996322e-06\n[12,] -2.235365e-06\n[13,] -1.796645e-06\n\n$DLTC\n             DLINPC\n [1,]  0.000000e+00\n [2,] -3.153787e-04\n [3,]  3.296896e-06\n [4,]  5.494574e-05\n [5,] -5.578570e-05\n [6,] -1.130090e-04\n [7,] -6.435459e-05\n [8,] -1.035859e-05\n [9,] -7.181135e-06\n[10,] -1.226398e-05\n[11,] -1.211890e-05\n[12,] -6.687646e-06\n[13,] -2.780821e-06\n\n$DLCETE28\n             DLINPC\n [1,]  0.000000e+00\n [2,] -2.169372e-04\n [3,] -1.794693e-04\n [4,] -9.599743e-05\n [5,] -3.611856e-05\n [6,] -1.002091e-05\n [7,] -6.148705e-06\n [8,] -6.463096e-06\n [9,] -1.112069e-05\n[10,] -5.734429e-06\n[11,] -3.411808e-06\n[12,] -1.150670e-06\n[13,] -1.268323e-06\n\n$DLIGAE\n             DLINPC\n [1,]  0.000000e+00\n [2,] -4.106588e-04\n [3,] -4.492109e-04\n [4,] -1.509848e-04\n [5,] -2.782137e-05\n [6,] -3.476186e-05\n [7,] -4.791977e-05\n [8,] -3.315994e-05\n [9,] -9.560203e-06\n[10,] -4.507522e-06\n[11,] -5.786606e-06\n[12,] -5.106278e-06\n[13,] -3.316415e-06\n\n$DLIPI\n             DLINPC\n [1,]  0.000000e+00\n [2,] -1.895883e-04\n [3,] -2.882596e-04\n [4,] -1.509791e-04\n [5,] -1.176876e-04\n [6,] -3.763557e-05\n [7,] -1.253013e-05\n [8,] -9.331166e-06\n [9,] -8.117816e-06\n[10,] -5.958131e-06\n[11,] -3.519211e-06\n[12,] -1.431827e-06\n[13,] -9.106440e-07\n\n\nUpper Band, CI= 0.95 \n$DLINPC\n            DLINPC\n [1,] 2.457783e-03\n [2,] 9.929182e-04\n [3,] 5.116489e-04\n [4,] 2.908694e-04\n [5,] 1.500842e-04\n [6,] 1.057040e-04\n [7,] 6.429016e-05\n [8,] 3.033504e-05\n [9,] 1.300678e-05\n[10,] 7.419926e-06\n[11,] 5.843143e-06\n[12,] 3.640856e-06\n[13,] 1.662979e-06\n\n$DLTC\n            DLINPC\n [1,] 0.000000e+00\n [2,] 1.857140e-04\n [3,] 5.466806e-04\n [4,] 4.418434e-04\n [5,] 1.627903e-04\n [6,] 2.684403e-05\n [7,] 2.560848e-05\n [8,] 4.282081e-05\n [9,] 3.281147e-05\n[10,] 1.315724e-05\n[11,] 4.767316e-06\n[12,] 3.938055e-06\n[13,] 4.889232e-06\n\n$DLCETE28\n            DLINPC\n [1,] 0.000000e+00\n [2,] 2.576404e-04\n [3,] 4.042545e-04\n [4,] 2.013296e-04\n [5,] 1.325918e-04\n [6,] 1.001898e-04\n [7,] 5.099291e-05\n [8,] 2.004539e-05\n [9,] 8.113697e-06\n[10,] 3.466024e-06\n[11,] 4.041302e-06\n[12,] 2.722010e-06\n[13,] 1.466011e-06\n\n$DLIGAE\n             DLINPC\n [1,]  0.000000e+00\n [2,]  4.377987e-05\n [3,] -9.875121e-06\n [4,]  8.871071e-05\n [5,]  1.420566e-04\n [6,]  8.320598e-05\n [7,]  2.111666e-05\n [8,]  1.501842e-05\n [9,]  1.505708e-05\n[10,]  1.537957e-05\n[11,]  7.057303e-06\n[12,]  2.055919e-06\n[13,]  2.011881e-06\n\n$DLIPI\n            DLINPC\n [1,] 0.000000e+00\n [2,] 3.027329e-04\n [3,] 1.454277e-04\n [4,] 7.271770e-05\n [5,] 6.600380e-05\n [6,] 4.412187e-05\n [7,] 3.566546e-05\n [8,] 2.280858e-05\n [9,] 9.979933e-06\n[10,] 3.222229e-06\n[11,] 2.898841e-06\n[12,] 2.761243e-06\n[13,] 1.387534e-06\n```\n\n\n:::\n\n```{.r .cell-code}\n#plot(IR_DLINPC)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nIR_DLTC <- irf(VAR_p, n.ahead = 12, boot = TRUE, \n               ci = 0.95, response = \"DLTC\")\n\nplot(IR_DLTC)\n```\n\n::: {.cell-output-display}\n![Impulso - Respuesta en $DLTC_t$](index_files/figure-html/fig64-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![Impulso - Respuesta en $DLTC_t$](index_files/figure-html/fig64-2.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![Impulso - Respuesta en $DLTC_t$](index_files/figure-html/fig64-3.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![Impulso - Respuesta en $DLTC_t$](index_files/figure-html/fig64-4.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![Impulso - Respuesta en $DLTC_t$](index_files/figure-html/fig64-5.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\nLos resultados muestran que la respuesta de $DLTC_t$ ante impulsos en los términos de error fue estadísticamente significativo sólo para alguunos de los casos y en periodos cortos de tiempo. El resto de los resultados de Impulso-Respuesta se encuentra en el Scrip llamado Clase 15 que se ubica en el repositorio de GitHub.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}